{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b68013b3377c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import math_ops\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import losses\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"MRdata.csv\")\n",
    "X_train = data.iloc[:, 0:1]\n",
    "y_train = data.iloc[:, 3:4]\n",
    "\n",
    "y_train_w_noise = data.iloc[:, 4:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            y_pred = test_tm_pred(self, x)\n",
    "#             y_pred = self(x, training=True)\n",
    "            # Compute our own loss\n",
    "            loss = keras.losses.mean_squared_error(y, y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Compute our own metrics\n",
    "        loss_tracker.update_state(loss)\n",
    "        mae_metric.update_state(y, y_pred)\n",
    "        return {\"loss\": loss_tracker.result(), \"mae\": mae_metric.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "                # We list our `Metric` objects here so that `reset_states()` can be\n",
    "                # called automatically at the start of each epoch\n",
    "                # or at the start of `evaluate()`.\n",
    "                # If you don't implement this property, you have to call\n",
    "                # `reset_states()` yourself at the time of your choosing.\n",
    "        return [loss_tracker, mae_metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(1,))\n",
    "output_1 = Dense(1)(input_1)\n",
    "\n",
    "input_2 = Input(shape=(1,))\n",
    "output_2 = Dense(1)(input_2)\n",
    "\n",
    "output_1 = Dense(1)(output_1)\n",
    "output_2 = Dense(1)(output_2)\n",
    "\n",
    "concat_2 = tf.keras.layers.Concatenate()([output_1, output_2])\n",
    "output_2 = Dense(1)(concat_2)\n",
    "model1 = CustomModel(inputs=[input_1, input_2], outputs=[output_2])\n",
    "\n",
    "layerCount = len(model1.layers)\n",
    "lastLayer = layerCount - 1;\n",
    "\n",
    "lastWeights = model1.layers[lastLayer].get_weights()\n",
    "lastWeights[0][0] = 1\n",
    "lastWeights[0][1] = 1\n",
    "lastWeights[1][0] = 0\n",
    "\n",
    "model1.layers[lastLayer].set_weights(lastWeights)\n",
    "\n",
    "model1.layers[lastLayer].trainable = False\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(0.02)\n",
    "# We don't passs a loss or metrics here.\n",
    "model1.compile(optimizer=opt, loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tm pred\n",
    "def test_tm_pred(self, x):\n",
    "        temp_0 = x[0]\n",
    "        temp_1 = x[1]\n",
    "        \n",
    "        x_0 = x[0]**2 + 2/x[0]\n",
    "        x_1 = 2*x[1] + 1/x[1]/x[1]\n",
    "        num_step = 30\n",
    "        increment_0 = x_0/num_step\n",
    "        increment_1 = x_1/num_step\n",
    "        start_0 = x_0*0\n",
    "        start_1 = x_1*0\n",
    "        start = [start_0, start_1]\n",
    "        \n",
    "        layers4 = self.layers[4]\n",
    "        layers5 = self.layers[5]\n",
    "        \n",
    "        ans_0 = layers4(start[0])\n",
    "        ans_1 = layers5(start[1])\n",
    "    \n",
    "        ans1 = 0\n",
    "        ans2 = 0\n",
    "        \n",
    "        for i in range(num_step):\n",
    "            _x = start\n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(_x)\n",
    "                _y = self(_x)\n",
    "\n",
    "            dy_dx = t.gradient(_y, _x)   \n",
    "    ### x2\n",
    "            ans1 = ans1 + dy_dx[0]*dy_dx[0]*increment_0\n",
    "            ans2 = ans2 + dy_dx[1]*dy_dx[1]*increment_1\n",
    "    ### x2\n",
    "    ### soft\n",
    "#         ans1 = ans1 + tf.math.log(tf.math.pow(2.0, dy_dx[0]) + 1)/tf.math.log(2.0)*increment_0\n",
    "#         ans2 = ans2 + tf.math.log(tf.math.pow(2.0, dy_dx[1]) + 1)/tf.math.log(2.0)*increment_1\n",
    "    ### soft\n",
    "            start_0 = start_0 + increment_0\n",
    "            start_1 = start_1 + increment_1\n",
    "            start = [start_0, start_1]\n",
    "#         return ans + ans1*2*(temp_0**2 - 1/temp_0) + ans2*2*(1/temp_1/temp_1 - temp_1)\n",
    "#         2*temp_0*(temp_0-1/temp_0/temp_0)*(ans_increment1+1/temp_0*ans_increment2)\n",
    "        return 2*temp_0*(temp_0-1/temp_0/temp_0)*(ans1+1/temp_0*ans2) + ans_1*2*(temp_0 - 1/temp_0/temp_0) + ans_0*2*(temp_0**2 - 1/temp_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit([X_train.to_numpy(), X_train.to_numpy()], y_train.to_numpy(), epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test tm pred\n",
    "def tm_pred(model, x):\n",
    "        temp_0 = x[0]\n",
    "        temp_1 = temp_0\n",
    "        \n",
    "        x[0] = x[0]**2 + 2/x[0]\n",
    "        x[1] = 2*x[1] + 1/x[1]/x[1]\n",
    "        \n",
    "        num_step = 10\n",
    "        increment_0 = x[0]/num_step\n",
    "        increment_1 = x[1]/num_step\n",
    "        start_0 = x[0]*0\n",
    "        start_1 = x[1]*0\n",
    "        start = [start_0, start_1]\n",
    "        ans_0 = model(start, training=True)\n",
    "        \n",
    "        ##\n",
    "        layerCount = len(model.layers)\n",
    "        lastLayer = layerCount - 1;\n",
    "\n",
    "        ## \n",
    "        lastWeights = model.layers[lastLayer].get_weights()\n",
    "        lastWeights[0][0] = 1\n",
    "        lastWeights[0][1] = 0\n",
    "\n",
    "        model.layers[lastLayer].set_weights(lastWeights)\n",
    "\n",
    "        ans_0 = model(start, training=True)\n",
    "        \n",
    "        ## \n",
    "        lastWeights[0][0] = 0\n",
    "        lastWeights[0][1] = 1\n",
    "\n",
    "        model.layers[lastLayer].set_weights(lastWeights)\n",
    "\n",
    "        ans_1 = model(start, training=True)\n",
    "        \n",
    "        lastWeights[0][0] = 1\n",
    "        lastWeights[0][1] = 1\n",
    "\n",
    "        model.layers[lastLayer].set_weights(lastWeights)\n",
    "\n",
    "        ##\n",
    "        values = []\n",
    "        \n",
    "        ans_increment1 = 0\n",
    "        ans_increment2 = 0\n",
    "        for i in range(num_step):\n",
    "            _x = start\n",
    "            _x = [list(_x[0]), list(_x[1])]\n",
    "            _x = [tf.constant(_x[0]), tf.constant(_x[1])]\n",
    "            \n",
    "            with tf.GradientTape() as t:\n",
    "                t.watch(_x)\n",
    "                _y = model(_x)\n",
    "\n",
    "            dy_dx = t.gradient(_y, _x)\n",
    "                ### x2\n",
    "            ans_increment1 = ans_increment1 + dy_dx[0]*dy_dx[0]*increment_0\n",
    "            ans_increment2 = ans_increment2 + dy_dx[1]*dy_dx[1]*increment_1\n",
    "                ### x2\n",
    "                ### soft\n",
    "#             ans_increment1 = ans_increment1 + tf.math.log(tf.math.pow(2.0, dy_dx[0]) + 1)/tf.math.log(2.0)*increment_0\n",
    "#             ans_increment2 = ans_increment2 + tf.math.log(tf.math.pow(2.0, dy_dx[1]) + 1)/tf.math.log(2.0)*increment_1\n",
    "                ### soft\n",
    "                \n",
    "            values.append(ans_increment1)\n",
    "            \n",
    "            start_0 = start_0 + increment_0\n",
    "            start_1 = start_1 + increment_1\n",
    "            start = [start_0, start_1]\n",
    "#         ans_increment = ans_increment1*2*(temp_0**2 - 1/temp_0) + ans_increment2*2*(1/temp_1/temp_1 - temp_1)\n",
    "        ans_increment = 2*temp_0*(temp_0-1/temp_0/temp_0)*(ans_increment1+1/temp_0*ans_increment2)\n",
    "        ans_increment = tf.reshape(ans_increment, ans_0.shape)\n",
    "        ans_increment = tf.dtypes.cast(ans_increment, tf.float32)\n",
    "#         return ans + ans_increment\n",
    "        return ans_increment + ans_1*2*(temp_0 - 1/temp_0/temp_0) + ans_0*2*(temp_0**2 - 1/temp_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1 = tm_pred(model1, [X_train.to_numpy(), X_train.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train.to_numpy(), y_train, label='train data')\n",
    "plt.plot(X_train.to_numpy(), predict1, label='fitted convex model')\n",
    "plt.legend()\n",
    "# plt.title('2_l_2_n_exp_combined_NN')\n",
    "# plt.xlabel('I1')\n",
    "\n",
    "# plt.xlabel('lambda')\n",
    "# plt.ylabel('Cauchy stress')\n",
    "plt.savefig('result/correct_MR/2_l_2_n_square_MR.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"MATLAB_figure_data/MR/square/y_train.csv\", y_train, delimiter=\",\")\n",
    "np.savetxt(\"MATLAB_figure_data/MR/square/y_prediction.csv\", predict1, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit([X_train.to_numpy(), X_train.to_numpy()], y_train_w_noise.to_numpy(), epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict1 = tm_pred(model1, [X_train.to_numpy(), X_train.to_numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train.to_numpy(), y_train_w_noise, label='train data', s=5, c='r', alpha=1)\n",
    "plt.plot(X_train.to_numpy(), predict1, label='fitted convex model')\n",
    "plt.legend()\n",
    "# plt.title('2_l_2_n_exp_combined_NN')\n",
    "# plt.xlabel('I1')\n",
    "\n",
    "# plt.xlabel('lambda')\n",
    "# plt.ylabel('Cauchy stress')\n",
    "plt.savefig('result/correct_MR/2_l_2_n_square_MR_noise.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"MATLAB_figure_data/MR/square/y_train_noise.csv\", y_train_w_noise, delimiter=\",\")\n",
    "np.savetxt(\"MATLAB_figure_data/MR/square/y_prediction_noise.csv\", predict1, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
